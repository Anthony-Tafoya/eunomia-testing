{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and GPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Utils.dbutils as dbutils\n",
    "import Utils.data_utils as data_utils\n",
    "from Utils.embedding_utils import train_embedding\n",
    "import Generators.CohortGenerator as CohortGenerator\n",
    "import Generators.FeatureGenerator as FeatureGenerator\n",
    "import Models.LogisticRegression.RegressionGen as lr_models\n",
    "import Models.Transformer.visit_transformer as visit_transformer\n",
    "import config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ipywidgets import IntProgress, FloatText\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.size\"] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "visit_transformer = importlib.reload(visit_transformer)\n",
    "data_utils = importlib.reload(data_utils)\n",
    "assert(torch.cuda.is_available())\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort, Outcome and Feature Collection\n",
    "\n",
    "### 1. Set up a connection to the OMOP CDM database\n",
    "\n",
    "Parameters for connection to be specified in ./config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection\n",
    "username = config.PG_USERNAME\n",
    "password = config.PG_PASSWORD\n",
    "database_name = config.DB_NAME\n",
    "\n",
    "config_path = 'postgresql://{username}:{password}@{database_name}'.format(\n",
    "    username = username,\n",
    "    password = password,\n",
    "    database_name = database_name\n",
    ")\n",
    "\n",
    "# schemas \n",
    "schema_name = 'eol_test' # all created tables will be created using this schema\n",
    "\n",
    "# caching\n",
    "reset_schema = False # if true, rebuild all data from scratch\n",
    "\n",
    "# set up database, reset schemas as needed\n",
    "db = dbutils.Database(config_path, schema_name)\n",
    "if reset_schema:\n",
    "    db.execute(\n",
    "        'drop schema if exists {} cascade'.format(schema_name)\n",
    "    )\n",
    "db.execute(\n",
    "    'create schema if not exists {}'.format(schema_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate the Cohort as per the given SQL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'eol_cohort'\n",
    "cohort_script_path = config.SQL_PATH_COHORTS + '/gen_EOL_cohort.sql'\n",
    "\n",
    "# cohort parameters  \n",
    "params = {\n",
    "          'cohort_table_name'     : cohort_name,\n",
    "          'schema_name'           : schema_name,\n",
    "          'aux_data_schema'       : config.CDM_AUX_SCHEMA,\n",
    "          'training_start_date'   : '2016-01-01',\n",
    "          'training_end_date'     : '2017-01-01',\n",
    "          'gap'                   : '3 months',\n",
    "          'outcome_window'        : '6 months'\n",
    "         }\n",
    "\n",
    "cohort = CohortGenerator.Cohort(\n",
    "    schema_name=schema_name,\n",
    "    cohort_table_name=cohort_name,\n",
    "    cohort_generation_script=cohort_script_path,\n",
    "    cohort_generation_kwargs=params,\n",
    "    outcome_col_name='y'\n",
    ")\n",
    "cohort.build(db, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate and build a feature set for each patient in the cohort using some default features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSet = FeatureGenerator.FeatureSet(db)\n",
    "featureSet.add_default_features(\n",
    "    ['drugs','conditions','procedures','specialty'],\n",
    "    schema_name,\n",
    "    cohort_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build the Feature Set by executing SQL queries and reading into sparse matrices\n",
    "cache_data_path = '/tmp/cache_data_eol_test'\n",
    "featureSet.build(cohort, from_cached=False, cache_file=cache_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Process the collected data and calculate indices needed for the deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_filt, feature_matrix_3d_transpose, remap, good_feature_names = \\\n",
    "    FeatureGenerator.postprocess_feature_matrix(cohort, featureSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data for deep model\n",
    "person_ixs, time_ixs, code_ixs = feature_matrix_3d_transpose.coords\n",
    "all_codes_tensor = code_ixs\n",
    "people = sorted(np.unique(person_ixs))\n",
    "person_indices = np.searchsorted(person_ixs, people)\n",
    "person_indices = np.append(person_indices, len(person_ixs))\n",
    "person_chunks = [\n",
    "    time_ixs[person_indices[i]: person_indices[i + 1]]\n",
    "    for i in range(len(person_indices) - 1)\n",
    "]\n",
    "\n",
    "visit_chunks = []\n",
    "visit_times_raw = []\n",
    "\n",
    "for i, chunk in enumerate(person_chunks):\n",
    "    visits = sorted(np.unique(chunk))\n",
    "    visit_indices_local = np.searchsorted(chunk, visits)\n",
    "    visit_indices_local = np.append(\n",
    "        visit_indices_local,\n",
    "        len(chunk)\n",
    "    )\n",
    "    visit_chunks.append(visit_indices_local)\n",
    "    visit_times_raw.append(visits)\n",
    "\n",
    "n_visits = {i:len(j) for i,j in enumerate(visit_times_raw)}\n",
    "\n",
    "visit_days_rel = {\n",
    "    i: (\n",
    "        pd.to_datetime(params['training_end_date']) \\\n",
    "        - pd.to_datetime(featureSet.time_map[i])\n",
    "    ).days for i in featureSet.time_map\n",
    "}\n",
    "vdrel_func = np.vectorize(visit_days_rel.get)\n",
    "visit_time_rel = [\n",
    "    vdrel_func(v) for v in visit_times_raw\n",
    "]\n",
    "\n",
    "maps = {\n",
    "    'concept': featureSet.concept_map,\n",
    "    'id': featureSet.id_map,\n",
    "    'time': featureSet.time_map\n",
    "}\n",
    "\n",
    "dataset_dict = {\n",
    "    'all_codes_tensor': all_codes_tensor, # A tensor of all codes occurring in the dataset\n",
    "    'person_indices': person_indices, # A list of indices such that all_codes_tensor[person_indices[i]: person_indices[i+1]] are the codes assigned to the ith patient\n",
    "    'visit_chunks': visit_chunks, # A list of indices such that all_codes_tensor[person_indices[i]+visit_chunks[j]:person_indices[i]+visit_chunks[j+1]] are the codes assigned to the ith patient during their jth visit\n",
    "    'visit_time_rel': visit_time_rel, # A list of times (as measured in days to the prediction date) for each visit\n",
    "    'n_visits': n_visits, # A dict defined such that n_visits[i] is the number of visits made by the ith patient\n",
    "    'outcomes_filt': outcomes_filt, # A pandas Series defined such that outcomes_filt.iloc[i] is the outcome of the ith patient\n",
    "    'remap': remap,\n",
    "    'maps': maps\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the windowed regression model on the task defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect featre names\n",
    "good_feature_names = np.vectorize(dataset_dict['maps']['concept'].get)(\n",
    "    dataset_dict['remap']['concept']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window the data using the window lengths specified below\n",
    "feature_matrix_counts, feature_names = data_utils.window_data_sorted(\n",
    "    window_lengths = [30, 180, 365, 730, 10000],\n",
    "    feature_matrix = feature_matrix_3d_transpose,\n",
    "    all_feature_names = good_feature_names,\n",
    "    cohort = cohort, featureSet = featureSet\n",
    ")\n",
    "feature_matrix_counts = feature_matrix_counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validate and test sets\n",
    "val_size = 5000\n",
    "indices_all = range(len(dataset_dict['outcomes_filt']))\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
    "    feature_matrix_counts, dataset_dict['outcomes_filt'], indices_all,\n",
    "    test_size=0.2, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the regression model over several choices of regularization parameter\n",
    "reg_lambdas = [2, 0.2, 0.02]\n",
    "lr_val_aucs = []\n",
    "for reg_lambda in reg_lambdas:\n",
    "    clf_lr = lr_models.gen_lr_pipeline(reg_lambda)\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    pred_lr = clf_lr.predict_proba(X_test[:val_size])[:, 1]\n",
    "    lr_val_aucs.append(roc_auc_score(y_test[:val_size], pred_lr))\n",
    "    print('Validation AUC: {0:.3f}'.format(roc_auc_score(y_test[:val_size], pred_lr)))\n",
    "                       \n",
    "clf_lr = lr_models.gen_lr_pipeline(reg_lambdas[np.argmax(lr_val_aucs)])\n",
    "clf_lr.fit(X_train, y_train)\n",
    "pred_lr_all = clf_lr.predict_proba(feature_matrix_counts)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the model with the best regularization, as measured by validation performance\n",
    "pred_lr = clf_lr.predict_proba(X_test[val_size:])[:, 1]\n",
    "print('Linear Model Test AUC: {0:.3f}'.format(roc_auc_score(y_test[val_size:], pred_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn a Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300 # size of embedding, must be multiple of number of heads\n",
    "window_days = 90 # number of days in window that defines a \"Sentence\" when learning the embedding\n",
    "train_coords = np.nonzero(np.where(np.isin(person_ixs, indices_train), 1, 0))\n",
    "embedding_filename = train_embedding(featureSet, feature_matrix_3d_transpose, window_days, \\\n",
    "                                     person_ixs[train_coords], time_ixs[train_coords], \\\n",
    "                                     remap['good_time_ixs'], embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the SARD deep model on the predictive task\n",
    "### 1. Set Model Parameters and Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same split as before, create train/validate/test batches for the deep model\n",
    "# `mbsz` might need to be decreased based on the GPU's memory and the number of features being used\n",
    "mbsz = 50\n",
    "def get_batches(arr, mbsz=mbsz):\n",
    "    curr, ret = 0, []\n",
    "    while curr < len(arr) - 1:\n",
    "        ret.append(arr[curr : curr + mbsz])\n",
    "        curr += mbsz\n",
    "    return ret\n",
    "\n",
    "p_ranges_train, p_ranges_test = [\n",
    "    get_batches(arr) for arr in (\n",
    "        indices_train, indices_test\n",
    "    )\n",
    "]\n",
    "p_ranges_val = p_ranges_test[:val_size // mbsz]\n",
    "p_ranges_test = p_ranges_test[val_size // mbsz:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a name for the model (mn_prefix) that will be used when saving checkpoints\n",
    "# Then, set some parameters for SARD. The values below reflect a good starting point that performed well on several tasks\n",
    "mn_prefix = 'eol_experiment_prefix'\n",
    "n_heads = 2\n",
    "assert embedding_dim % n_heads == 0\n",
    "model_params = {\n",
    "    'embedding_dim': int(embedding_dim / n_heads), # Dimension per head of visit embeddings\n",
    "    'n_heads': n_heads, # Number of self-attention heads\n",
    "    'attn_depth': 2, # Number of stacked self-attention layers\n",
    "    'dropout': 0.05, # Dropout rate for both self-attention and the final prediction layer\n",
    "    'use_mask': True, # Only allow visits to attend to other actual visits, not to padding visits\n",
    "    'concept_embedding_path': embedding_filename # if unspecified, uses default Torch embeddings\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up fixed model parameters, loss functions, and build the model on the GPU\n",
    "lr = 2e-4\n",
    "n_epochs_pretrain = 1\n",
    "ft_epochs = 1\n",
    "\n",
    "update_every = 500\n",
    "update_mod = update_every // mbsz\n",
    "\n",
    "base_model = visit_transformer.VisitTransformer(\n",
    "    featureSet, **model_params\n",
    ")\n",
    "\n",
    "clf = visit_transformer.VTClassifer(base_model).cuda()\n",
    "\n",
    "clf.bert.set_data(\n",
    "    torch.LongTensor(dataset_dict['all_codes_tensor']).cuda(),\n",
    "    dataset_dict['person_indices'], dataset_dict['visit_chunks'],\n",
    "    dataset_dict['visit_time_rel'], dataset_dict['n_visits']\n",
    ")\n",
    "\n",
    "loss_function_distill = torch.nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.FloatTensor([\n",
    "        len(dataset_dict['outcomes_filt']) / dataset_dict['outcomes_filt'].sum() - 1\n",
    "    ]), reduction='sum'\n",
    ").cuda()\n",
    "\n",
    "optimizer_clf = torch.optim.Adam(params=clf.parameters(), lr=lr)\n",
    "\n",
    "def eval_curr_model_on(a):\n",
    "    with torch.no_grad():\n",
    "        preds_test, true_test = [], []\n",
    "        for batch_num, p_range in enumerate(a):\n",
    "            y_pred = clf(p_range) \n",
    "            preds_test += y_pred.tolist()\n",
    "            true_test += list(dataset_dict['outcomes_filt'].iloc[list(p_range)].values)\n",
    "        return roc_auc_score(true_test, preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fit the SARD model to the best windowed linear model (Reverse Distillation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `n_epochs_pretrain` of Reverse Distillation pretraining\n",
    "val_losses = []\n",
    "progress_bar = IntProgress(min=0, max=int(n_epochs_pretrain * len(p_ranges_train)))\n",
    "batch_loss_disp = FloatText(value=0.0, description='Avg. Batch Loss for Last 50 Batches', disabled=True)\n",
    "time_disp = FloatText(value=0.0, description='Time for Last 50 Batches', disabled=True)\n",
    "\n",
    "display(progress_bar)\n",
    "display(batch_loss_disp)\n",
    "display(time_disp)\n",
    "\n",
    "for epoch in range(n_epochs_pretrain):\n",
    "    t, batch_loss = time.time(), 0\n",
    "    \n",
    "    for batch_num, p_range in enumerate(p_ranges_train):\n",
    "        \n",
    "        if batch_num % 50 == 0:\n",
    "            batch_loss_disp.value = round(batch_loss / 50, 2)\n",
    "            time_disp.value = round(time.time() - t, 2)\n",
    "            t, batch_loss = time.time(), 0\n",
    "            \n",
    "        y_pred = clf(p_range)\n",
    "        loss_distill = loss_function_distill(\n",
    "            y_pred, torch.FloatTensor(pred_lr_all[p_range]).cuda()\n",
    "        )\n",
    "        \n",
    "        batch_loss += loss_distill.item()\n",
    "        loss_distill.backward()\n",
    "        \n",
    "        if batch_num % update_mod == 0:\n",
    "            optimizer_clf.step()\n",
    "            optimizer_clf.zero_grad()\n",
    "        \n",
    "        progress_bar.value = batch_num + epoch * len(p_ranges_train)\n",
    "        \n",
    "    torch.save(\n",
    "        clf.state_dict(),\n",
    "        \"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "                task=task, mn_prefix = mn_prefix, epochs = epoch + 1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    clf.eval()\n",
    "    ckpt_auc = eval_curr_model_on(p_ranges_val)\n",
    "    print('Epochs: {} | Val AUC: {}'.format(epoch + 1, ckpt_auc))\n",
    "    val_losses.append(ckpt_auc)\n",
    "    clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pretrained model with best validation-set performance\n",
    "clf.load_state_dict(\n",
    "    torch.load(\"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "        task=task, mn_prefix=mn_prefix, epochs=np.argmax(val_losses) + 1\n",
    "    ))\n",
    ")\n",
    "torch.save(\n",
    "        clf.state_dict(),\n",
    "        \"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "                task=task, mn_prefix = mn_prefix, epochs = 'BEST'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tune the SARD model by training to match the actual outcomes on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss functions for fine-tuning. There are two terms:\n",
    "#    - `loss_function_distill`, which penalizes differences between the linear model prediction and SARD's prediction\n",
    "#    - `loss_function_clf`, which penalizes differences between the true outcome and SARD's prediction\n",
    "loss_function_distill = torch.nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.FloatTensor([\n",
    "        len(dataset_dict['outcomes_filt']) / dataset_dict['outcomes_filt'].sum() - 1\n",
    "    ]), reduction='sum'\n",
    ").cuda()\n",
    "\n",
    "loss_function_clf = torch.nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.FloatTensor([\n",
    "        len(dataset_dict['outcomes_filt']) / dataset_dict['outcomes_filt'].sum() - 1\n",
    "    ]), reduction='sum'\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run `ft_epochs` of fine-tuning training, for each of the values of `alpha` below.\n",
    "# Note that `alpha` is the relative weight of `loss_function_distill` as compared to `loss_function_clf`\n",
    "\n",
    "all_pred_models = {}\n",
    "\n",
    "progress_bar = IntProgress(min=0, max=int(ft_epochs * len(p_ranges_train)))\n",
    "batch_loss_disp = FloatText(value=0.0, description='Avg. Batch Loss for Last 50 Batches', disabled=True)\n",
    "time_disp = FloatText(value=0.0, description='Time for Last 50 Batches', disabled=True)\n",
    "\n",
    "display(progress_bar)\n",
    "display(batch_loss_disp)\n",
    "display(time_disp)\n",
    "\n",
    "\n",
    "no_rd = False\n",
    "for alpha in [0,0.05,0.1,0.15, 0.2]:\n",
    "\n",
    "    progress_bar.value = 0\n",
    "    \n",
    "    \n",
    "    if no_rd:\n",
    "        pretrained_model_fn = mn_prefix + '_None'\n",
    "        start_model = None\n",
    "        if start_model is None:\n",
    "            base_model = visit_transformer.VisitTransformer(\n",
    "                featureSet, **model_params\n",
    "            )\n",
    "\n",
    "            clf = visit_transformer.VTClassifer(base_model).cuda()\n",
    "\n",
    "            clf.bert.set_data(\n",
    "                torch.LongTensor(dataset_dict['all_codes_tensor']).cuda(),\n",
    "                dataset_dict['person_indices'], dataset_dict['visit_chunks'],\n",
    "                dataset_dict['visit_time_rel'], dataset_dict['n_visits']\n",
    "            )\n",
    "        else:\n",
    "            pretrained_model_path = \"SavedModels/{task}/{start_model}\".format(\n",
    "                task=task, start_model=start_model\n",
    "            )\n",
    "            clf.load_state_dict(torch.load(pretrained_model_path))\n",
    "            \n",
    "    else: \n",
    "        pretrained_model_fn = \"{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "            mn_prefix=mn_prefix, epochs='BEST'\n",
    "        )\n",
    "        pretrained_model_path = \"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "            task=task, mn_prefix=mn_prefix, epochs='BEST'\n",
    "        )\n",
    "        clf.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "    clf.train()\n",
    "    \n",
    "    optimizer_clf = torch.optim.Adam(params=clf.parameters(), lr=2e-4)\n",
    "\n",
    "    for epoch in range(ft_epochs):\n",
    "\n",
    "        t, batch_loss = time.time(), 0\n",
    "\n",
    "        for batch_num, p_range in enumerate(p_ranges_train):\n",
    "\n",
    "            if batch_num % 50 == 0:\n",
    "                batch_loss_disp.value = round(batch_loss / 50, 2)\n",
    "                time_disp.value = round(time.time() - t, 2)\n",
    "                t, batch_loss = time.time(), 0\n",
    "\n",
    "            y_pred = clf(p_range)\n",
    "            \n",
    "            loss = loss_function_clf(\n",
    "                y_pred,\n",
    "                torch.FloatTensor(dataset_dict['outcomes_filt'].values[p_range]).cuda()\n",
    "            )\n",
    "\n",
    "            loss_distill = loss_distill = loss_function_distill(\n",
    "                y_pred,\n",
    "                torch.FloatTensor(pred_lr_all[p_range]).cuda()\n",
    "            )\n",
    "\n",
    "            batch_loss += loss.item() + alpha * loss_distill.item()\n",
    "            loss_total = loss + alpha * loss_distill\n",
    "            loss_total.backward()\n",
    "            \n",
    "            if batch_num % update_mod == 0:\n",
    "                optimizer_clf.step()\n",
    "                optimizer_clf.zero_grad()\n",
    "\n",
    "            progress_bar.value = batch_num + epoch * len(p_ranges_train)\n",
    "        \n",
    "        saving_fn = \"{pretrain}_alpha_{alpha}_epochs_{epochs}\".format(\n",
    "            task=task, pretrain = pretrained_model_fn, alpha=alpha, epochs = epoch + 1\n",
    "        )\n",
    "        torch.save(\n",
    "            clf.state_dict(),\n",
    "            \"SavedModels/{task}/ablation/{saving_fn}\".format(\n",
    "                    task=task, saving_fn=saving_fn\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        clf.eval()\n",
    "        val_auc = eval_curr_model_on(p_ranges_val)\n",
    "        print(val_auc)\n",
    "        all_pred_models[saving_fn] = val_auc\n",
    "        clf.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the best SARD model, as determined by validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = max(all_pred_models, key=all_pred_models.get()\n",
    "clf.load_state_dict(\n",
    "    torch.load(\"SavedModels/{task}/{model}\".format(\n",
    "        task=task, model=best_model\n",
    "    ))\n",
    ")\n",
    "clf.eval();\n",
    "with torch.no_grad():\n",
    "    preds_test, true_test = [], []\n",
    "    for batch_num, p_range in enumerate(p_ranges_test):\n",
    "        y_pred = clf(p_range) \n",
    "        preds_test += y_pred.tolist()\n",
    "        true_test += list(dataset_dict['outcomes_filt'].iloc[list(p_range)].values)\n",
    "    print(roc_auc_score(true_test, preds_test))\n",
    "clf.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl2kernel",
   "language": "python",
   "name": "pl2kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
